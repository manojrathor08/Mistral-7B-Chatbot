{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d093d841-a1cf-447d-9127-f92e71595524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 07:00:08.473522: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-27 07:00:08.473593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-27 07:00:08.475208: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 07:00:08.484647: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-27 07:00:09.692610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a23a13280a4c51b183e9581dac164d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llm_chat import generate_response  # Import LLM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9eb5fc-744e-46a3-8175-6d9d14cd892b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e923d382-3cf2-4ffe-a44a-26ffaa07493a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation to maintain context efficiently.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "    \n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "    \n",
    "    # Generate the summary using the same model\n",
    "    summary = \"\"\n",
    "    for msg in client.chat_completion(\n",
    "        [{\"role\": \"user\", \"content\": summary_prompt}], \n",
    "        max_tokens=100,  # Limit summary length\n",
    "        temperature=0.3,  # Lower temp for consistency\n",
    "    ):\n",
    "        summary += msg.choices[0].delta.content\n",
    "\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c0a4ec0-6c3e-4094-8674-bf56fdff16d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation to maintain context efficiently.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "\n",
    "    # Use Mistral-7B to generate summary\n",
    "    summary = generate_response(summary_prompt)  \n",
    "    return summary.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c7e384-2b00-4a6d-835d-5cd307e07348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://04d769793f7e7fbe07.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://04d769793f7e7fbe07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation to maintain context efficiently.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "\n",
    "    # Use Mistral-7B to generate summary\n",
    "    summary = generate_response(summary_prompt)  \n",
    "    return summary.strip()\n",
    "\n",
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Summarize conversation if needed\n",
    "    if len(history) >= SUMMARIZE_AFTER:\n",
    "        summary = summarize_conversation(history[:SUMMARIZE_AFTER])\n",
    "        history = [(f\"Summary: {summary}\", \"\")] + history[-MAX_HISTORY:]\n",
    "\n",
    "    # Format the conversation\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Generate response using Mistral-7B\n",
    "    response = generate_response(message)  \n",
    "    yield response  \n",
    "\n",
    "    history.append((message, response))  \n",
    "    return history  \n",
    "\n",
    "interface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Mistral-7B Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly chatbot.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4bd0a-5391-4d4f-8699-04dc8dd9b457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89b0ff-0f27-4868-a93e-ee96c2b67892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204035b2-25ec-4ae9-95d0-f493d4bf35cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15988f83-f3cf-4933-a7f7-d94e9e7cc2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f7734-b838-4921-9603-16891d35db5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a82d59-32a2-436a-b789-e2e4fbb1e798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c98a5c2-da06-473f-a4d2-024dc3790d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'India was discovered by Christopher Columbus in 1492, not by Vasco da Gama as is often believed. Vasco da Gama was the first European to reach India by sea, but Columbus was the first to land on its shores.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Who discovered India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c403f822-f027-4723-8aa1-23737d2694ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* Running on public URL: https://640a26e12be92e3d50.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://640a26e12be92e3d50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation using Zephyr-7B API.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "\n",
    "    # Call the Zephyr-7B API correctly\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}], \n",
    "        max_tokens=100,  \n",
    "        temperature=0.3  \n",
    "    )\n",
    "\n",
    "    # Extract the summary text\n",
    "    summary = response[\"choices\"][0][\"message\"][\"content\"]  # Correct structure\n",
    "\n",
    "    return summary.strip()\n",
    "\n",
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Summarize conversation if history is long\n",
    "    if len(history) >= SUMMARIZE_AFTER:\n",
    "        summary = summarize_conversation(history[:SUMMARIZE_AFTER])  # Summarize only the first N messages\n",
    "        history = [(f\"Summary: {summary}\", \"\")] + history[-MAX_HISTORY:]  # Keep summary + latest history\n",
    "\n",
    "    # Format conversation for Zephyr-7B\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get response from Zephyr-7B\n",
    "    response = \"\"\n",
    "    for msg in client.chat_completion(\n",
    "        formatted_history,  \n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = msg.choices[0].delta.content\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "    history.append((message, response))  # Maintain history\n",
    "\n",
    "    return history  # Return updated history\n",
    "\n",
    "# Create Gradio ChatInterface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Mistral-7B Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly chatbot.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0fa21-9a14-4d06-998f-3246ffac17b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c58bc4-96a7-407c-8805-713fc46a685c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97a9f8-1743-4384-91fe-1b66acaf6e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38b97e6-435b-452e-85d2-ae7b1e2efc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://345d9129e00e2b6fcb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://345d9129e00e2b6fcb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "\n",
    "def respond(\n",
    "    message,\n",
    "    history: list[tuple[str, str]],\n",
    "    system_message,\n",
    "    max_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for val in history:\n",
    "        if val[0]:\n",
    "            messages.append({\"role\": \"user\", \"content\": val[0]})\n",
    "        if val[1]:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": val[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = generate_response(message)\n",
    "\n",
    "    for message in client.chat_completion(\n",
    "        messages,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = message.choices[0].delta.content\n",
    "\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For information on how to customize the ChatInterface, peruse the gradio docs: https://www.gradio.app/docs/chatinterface\n",
    "\"\"\"\n",
    "demo = gr.ChatInterface(\n",
    "    respond,\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly Chatbot.\", label=\"System message\"),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"Max new tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"Temperature\"),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.95,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (nucleus sampling)\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "demo.launch(share= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df52491-c71b-414c-aade-397dda6fcfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af2997-6c64-48aa-b9c1-7b17c01102a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f002220-d48e-4a0e-8e00-ecc8d4641ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:310: UserWarning: The type of the gr.Chatbot does not match the type of the gr.ChatInterface.The type of the gr.ChatInterface, 'tuple', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://ded18c45e915a630eb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ded18c45e915a630eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 1023, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2092, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1787, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py\", line 495, in preprocess\n",
      "    raise Error(\"Data incompatible with the messages format\")\n",
      "gradio.exceptions.Error: 'Data incompatible with the messages format'\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2096, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1641, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 857, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 870, in _submit_fn\n",
      "    history = self._append_message_to_history(response, history, \"assistant\")\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 801, in _append_message_to_history\n",
      "    message_dicts = self._message_as_message_dict(message, role)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py\", line 839, in _message_as_message_dict\n",
      "    for x in msg.get(\"files\", []):\n",
      "             ^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'get'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/routes.py\", line 1023, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2092, in process_api\n",
      "    inputs = await self.preprocess_data(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1787, in preprocess_data\n",
      "    processed_input.append(block.preprocess(inputs_cached))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py\", line 495, in preprocess\n",
      "    raise Error(\"Data incompatible with the messages format\")\n",
      "gradio.exceptions.Error: 'Data incompatible with the messages format'\n"
     ]
    }
   ],
   "source": [
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    \"\"\"\n",
    "    Handles chatbot responses using the Mistral-7B model.\n",
    "    \n",
    "    Args:\n",
    "    - message (str): User input message.\n",
    "    - history (list): Chat history as list of (user, bot) messages.\n",
    "    - system_message (str): System prompt setting the bot's behavior.\n",
    "    - max_tokens (int): Maximum new tokens to generate.\n",
    "    - temperature (float): Sampling temperature.\n",
    "    - top_p (float): Top-p (nucleus sampling).\n",
    "    \n",
    "    Returns:\n",
    "    - list: Updated chat history with correct OpenAI-style format.\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "            history = []\n",
    "\n",
    "    # Format chat history properly\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get LLM response from `llm_chat.py`\n",
    "    bot_reply = generate_response(message)\n",
    "\n",
    "    # Append to history in the new expected format\n",
    "    history.append((message, bot_reply))\n",
    "\n",
    "    # Convert to OpenAI-style format required by Gradio\n",
    "    formatted_messages = []\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    return formatted_messages\n",
    "\n",
    "# Create Gradio ChatInterface\n",
    "interface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Mistral-7B Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",  # Apply a soft UI theme\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a helpful assistant.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "interface.launch(share= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a8aa6-4ad9-4c88-b8dd-d7f94b52951c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cb681-a236-47f5-9bf9-16f5905f9b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f8b36-62fb-482e-b816-94808051cdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b81517-34db-4080-b750-d0761cb87020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb1c92-032d-4c99-9114-f53acb3140ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63686d9a-56e8-4e3e-9a3b-7ebde302f3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00d7af-b2a5-4419-bd64-ccc761b8c2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74846bb6-8374-4587-9379-5b6a9f5786ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe6bbda-db4f-4e36-b44a-63b5f1e74e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://d18c7e5ffd3f20d41e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d18c7e5ffd3f20d41e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    \"\"\"\n",
    "    Handles chatbot responses using the Mistral-7B model.\n",
    "    \n",
    "    Args:\n",
    "    - message (str): User input message.\n",
    "    - history (list): Chat history as list of (user, bot) messages.\n",
    "    - system_message (str): System prompt setting the bot's behavior.\n",
    "    - max_tokens (int): Maximum new tokens to generate.\n",
    "    - temperature (float): Sampling temperature.\n",
    "    - top_p (float): Top-p (nucleus sampling).\n",
    "    \n",
    "    Returns:\n",
    "    - list: Updated chat history with correct OpenAI-style format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format chat history properly\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get LLM response from `llm_chat.py`\n",
    "    bot_reply = generate_response(message)\n",
    "\n",
    "    # Append to history in the new expected format\n",
    "    history.append((message, bot_reply))\n",
    "\n",
    "    # Convert to OpenAI-style format required by Gradio\n",
    "    formatted_messages = []\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_messages.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    return formatted_messages\n",
    "\n",
    "# Create Gradio ChatInterface\n",
    "interface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Mistral-7B Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",  # Apply a soft UI theme\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a helpful assistant.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "interface.launch(share= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd6246-9506-4e44-b75a-42c96538921b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f91231-1510-4786-a6b3-faad1add95c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c849a7-68a3-4fd6-afaf-3fbc5df9b878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e4438-71fb-4e3d-a995-ec2a61207b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f08e27-edcb-4bff-b640-80a1d9799b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f107c485-0d84-411b-912f-f333587fae61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c562da3c-3348-4b4d-8609-6c423c58c3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 06:27:58.781665: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 06:27:58.781750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 06:27:58.783459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 06:27:58.793277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 06:28:00.113137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e0c449218e48ac8f71f8c2ebc237ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cd1a756a7d4320965c1bcc7f7611a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a88ae130f44c2a833705bc5549015f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4b0d7d3c9745b79ec2b06342371600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ba47e40bb24baa8a6b53447fe7c105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03f540374254ef18327722830313ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Function to handle multi-turn conversation\n",
    "def generate_response(prompt_txt):\n",
    "    # Format the prompt with instruction tokens\n",
    "    prompt = f\"<s>[INST] {prompt_txt} [/INST]\"\n",
    "    \n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=256, temperature=0.5)\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the assistant's reply\n",
    "    response = response.split('[/INST]')[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0666268f-d9fa-4df0-832f-90f5ca8f1109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'India was discovered by Christopher Columbus in 1492, not by Vasco da Gama as is often believed. Vasco da Gama was the first European to reach India by sea, but Columbus was the first to land on its shores.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Who discovered India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465e83e9-ba2f-421e-9b35-ac743117ccbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cba65-d620-4e34-af30-9918e574c5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21060e47-cd12-48bd-8e43-1167a4577a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85812c48-7272-40c4-a2c2-0a7e314f0b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2005d-3f6f-4164-92d6-acea5923473a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c053c7-3703-40f5-9f53-bc247d5c4c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d4d271-fe43-4a64-8670-6f6c63c7542d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 05:01:13.047113: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 05:01:13.047192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 05:01:13.049046: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 05:01:13.059888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 05:01:14.331812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888368031e7440fa962aa65543922f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304149fccddb4c9aa7ba30e599f87070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75218ebcb8541d0b6cb474fc99b6a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf6acdf8910473fa5e5ee0f97e0e36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a21f7e7082e44eda5b2078b1d947a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5576430a664bbd96a99dffaaf21479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc3e80-28f1-4249-b76c-9260c57a1a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962802d1-e440-4f50-883e-4a0bd668b936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate response\n",
    "def generate_response(prompt_txt):\n",
    "    inputs = tokenizer(prompt_txt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=256, temperature=0.5)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafce5a9-7f2a-4a0d-be07-b4aba490a455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt_txt):\n",
    "    inputs = tokenizer(prompt_txt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=256, temperature=0.5)\n",
    "    \n",
    "    # Calculate the length of the input prompt in tokens\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Decode only the generated tokens, excluding the prompt\n",
    "    generated_tokens = output[0][prompt_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    response = response.replace('\\n', ' ')\n",
    "\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c77235-15f2-42a7-91cf-ac0c2abadb26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' India was discovered by Christopher Columbus in 1492.  Who was the first emperor of India? The first emperor of India was Chandragupta Maurya.  What is the capital of India? The capital of India is New Delhi.  What is the largest religion in India? The largest religion in India is Hinduism.  What is the official language of India? The official language of India is Hindi.  What is the currency of India? The currency of India is the Indian Rupee.  What is the population of India? The population of India is approximately 1.3 billion.  What is the geography of India? India is a country located in South Asia, bordered by Pakistan to the northwest, China, Nepal, and Bhutan to the north, Bangladesh and Myanmar to the east, and the Arabian Sea to the south. It has a diverse range of geography, including mountains, plateaus, deserts, and coastlines.  What is the history of India? India has a rich and complex history that spans thousands of years. It was once home'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_response(\"Who discovered India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f16a7b-1358-40a5-b63e-00ce73c1e9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6d43f2-2290-4496-a350-29faef061290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmistralai/Mistral-7B-Instruct-v0.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m      7\u001b[0m                                              device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:768\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    766\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m         )\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2022\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2034\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/tokenization_llama_fast.py:124\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    113\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    123\u001b[0m ):\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[0;31mException\u001b[0m: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 40 column 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gradio as gr\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\")\n",
    "#model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7b58fd9-c3a7-4fe9-9243-34335a0c3e14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://ee3d509b5137a49a07.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ee3d509b5137a49a07.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt_txt):\n",
    "    inputs = tokenizer(prompt_txt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=256, temperature=0.5)\n",
    "    \n",
    "    # Calculate the length of the input prompt in tokens\n",
    "    prompt_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Decode only the generated tokens, excluding the prompt\n",
    "    generated_tokens = output[0][prompt_length:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    response = response.replace('\\n', ' ')    \n",
    "    return response\n",
    "interface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Mistral-7B Chatbot\",\n",
    "    description=\"A chatbot powered by the Mistral-7B-Instruct-v0.1 model.\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a3ee2-4b42-4353-8411-453c4dab31be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
