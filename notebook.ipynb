{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d093d841-a1cf-447d-9127-f92e71595524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 08:20:46.878914: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-27 08:20:46.878993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-27 08:20:46.880983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-27 08:20:46.893514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-27 08:20:48.316638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76c76446810423db0224356392b2cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from llm_chat import generate_response  # Import LLM function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac4ae6-4cc6-49ef-82a3-fa06e56cc55d",
   "metadata": {},
   "source": [
    "# Mistral chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c7e384-2b00-4a6d-835d-5cd307e07348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://5d04053760fdd44350.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5d04053760fdd44350.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation to maintain context efficiently.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "\n",
    "    # Use Mistral-7B to generate summary\n",
    "    summary = generate_response(summary_prompt)  \n",
    "    return summary.strip()\n",
    "\n",
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Summarize conversation if needed\n",
    "    if len(history) >= SUMMARIZE_AFTER:\n",
    "        summary = summarize_conversation(history[:SUMMARIZE_AFTER])\n",
    "        history = [(f\"Summary: {summary}\", \"\")] + history[-MAX_HISTORY:]\n",
    "\n",
    "    # Format the conversation\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Generate response using Mistral-7B\n",
    "    response = generate_response(message)  \n",
    "    yield response  \n",
    "\n",
    "    history.append((message, response))  \n",
    "    return history  \n",
    "\n",
    "interface = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Mistral-7B Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly chatbot.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4bd0a-5391-4d4f-8699-04dc8dd9b457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394ab54a-07f2-4853-a324-dd9a9b7eeeca",
   "metadata": {},
   "source": [
    "# Zephyr Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f7734-b838-4921-9603-16891d35db5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c403f822-f027-4723-8aa1-23737d2694ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://24bd81660fe7b01b5b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://24bd81660fe7b01b5b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "MAX_HISTORY = 2  # Keep last 5 exchanges\n",
    "SUMMARIZE_AFTER = 3  # Summarize after every 6 exchanges\n",
    "\n",
    "def summarize_conversation(history):\n",
    "    \"\"\"\n",
    "    Summarizes the conversation using Zephyr-7B API.\n",
    "    \"\"\"\n",
    "    summary_prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        summary_prompt += f\"User: {user_msg}\\nAssistant: {bot_reply}\\n\"\n",
    "\n",
    "    summary_prompt += \"\\nSummary:\"\n",
    "\n",
    "    # Call the Zephyr-7B API correctly\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}], \n",
    "        max_tokens=100,  \n",
    "        temperature=0.3  \n",
    "    )\n",
    "\n",
    "    # Extract the summary text\n",
    "    summary = response[\"choices\"][0][\"message\"][\"content\"]  # Correct structure\n",
    "\n",
    "    return summary.strip()\n",
    "\n",
    "def respond(message, history, system_message, max_tokens, temperature, top_p):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    # Summarize conversation if history is long\n",
    "    if len(history) >= SUMMARIZE_AFTER:\n",
    "        summary = summarize_conversation(history[:SUMMARIZE_AFTER])  # Summarize only the first N messages\n",
    "        history = [(f\"Summary: {summary}\", \"\")] + history[-MAX_HISTORY:]  # Keep summary + latest history\n",
    "\n",
    "    # Format conversation for Zephyr-7B\n",
    "    formatted_history = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    for user_msg, bot_reply in history:\n",
    "        formatted_history.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        formatted_history.append({\"role\": \"assistant\", \"content\": bot_reply})\n",
    "\n",
    "    formatted_history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    # Get response from Zephyr-7B\n",
    "    response = \"\"\n",
    "    for msg in client.chat_completion(\n",
    "        formatted_history,  \n",
    "        max_tokens=max_tokens,\n",
    "        stream=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    ):\n",
    "        token = msg.choices[0].delta.content\n",
    "        response += token\n",
    "        yield response\n",
    "\n",
    "    history.append((message, response))  # Maintain history\n",
    "\n",
    "    return history  # Return updated history\n",
    "\n",
    "# Create Gradio ChatInterface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"Zephyr Chatbot ü§ñ\",\n",
    "    description=\"üí¨ Chat with a fine-tuned Mistral-7B model for interactive conversations.\",\n",
    "    theme=\"soft\",\n",
    "    additional_inputs=[\n",
    "        gr.Textbox(value=\"You are a friendly chatbot.\", label=\"üõ† System Message\", interactive=True),\n",
    "        gr.Slider(minimum=1, maximum=2048, value=512, step=1, label=\"üìè Max Tokens\"),\n",
    "        gr.Slider(minimum=0.1, maximum=4.0, value=0.7, step=0.1, label=\"üî• Temperature\"),\n",
    "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, step=0.05, label=\"üéØ Top-p Sampling\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0fa21-9a14-4d06-998f-3246ffac17b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
